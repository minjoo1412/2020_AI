{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제4정답.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjoo1412/2020_AI/blob/master/%EA%B3%BC%EC%A0%9C4%EC%A0%95%EB%8B%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGi5TXWj_IzI"
      },
      "source": [
        "# 3번"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz49HZCpAeiy",
        "outputId": "3095b2d3-2fcf-4759-a0cf-8b417e51455c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77J59VLb_FqZ"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA06z2iIBGtg",
        "outputId": "b4640aa2-b369-41a6-b81e-7b7c8b2b4b94"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "print(findFiles('names/*.txt'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['names/English.txt', 'names/Irish.txt', 'names/Scottish.txt', 'names/French.txt', 'names/Japanese.txt', 'names/Vietnamese.txt', 'names/Dutch.txt', 'names/Chinese.txt', 'names/Czech.txt', 'names/Spanish.txt', 'names/German.txt', 'names/Polish.txt', 'names/Italian.txt', 'names/Russian.txt', 'names/Portuguese.txt', 'names/Korean.txt', 'names/Greek.txt', 'names/Arabic.txt']\n",
            "Slusarski\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU437rsxBcL5",
        "outputId": "38ece8e0-4827-46b8-c2c9-ff2981eecd55"
      },
      "source": [
        "print(category_lines['Italian'][:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z_rpwcxCN5Q",
        "outputId": "366b0f58-f6fe-44e9-b145-f826c90f7a97"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('J'))\n",
        "\n",
        "print(lineToTensor('Jones').size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "torch.Size([5, 1, 57])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq8ofapaCVQx"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKkY_eECX2U"
      },
      "source": [
        "input = letterToTensor('A')\n",
        "hidden =torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input, hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIOZ2EMkCZbS",
        "outputId": "41b405f2-d89e-4d65-bf25-e361a7ad0b6a"
      },
      "source": [
        "input = lineToTensor('Albert')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input[0], hidden)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.8462, -2.8547, -2.9517, -2.7451, -2.8619, -2.8903, -2.8562, -2.9349,\n",
            "         -2.9847, -2.9299, -2.9012, -2.9031, -2.9425, -2.8920, -2.8757, -2.9362,\n",
            "         -2.8001, -2.9507]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dvw6jbFCa5d",
        "outputId": "f797df35-c9a8-4622-f500-4f1b571892c6"
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('French', 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mprau3hnCcf1",
        "outputId": "78bda187-a04b-429c-9c4e-bd654b07bc2c"
      },
      "source": [
        "import random\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category = Greek / line = Agelakos\n",
            "category = Arabic / line = Baba\n",
            "category = German / line = Aleshite\n",
            "category = Greek / line = Christodoulou\n",
            "category = Chinese / line = Moy\n",
            "category = Russian / line = Piskorsky\n",
            "category = Russian / line = Ajnikov\n",
            "category = Italian / line = Abramo\n",
            "category = Vietnamese / line = Truong\n",
            "category = Chinese / line = Min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHi1SzOhCeYH"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    return output, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZNvxuZ7CiBo",
        "outputId": "85a46367-1a3e-4164-8c6c-7ec5cc0bb354"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(category_tensor, line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5000 5% (0m 7s) 1.5604 Phocas / Greek ✓\n",
            "10000 10% (0m 15s) 1.0808 Achteren / Dutch ✓\n",
            "15000 15% (0m 22s) 3.7353 Kann / Korean ✗ (Dutch)\n",
            "20000 20% (0m 30s) 2.2151 Vodden / Dutch ✗ (English)\n",
            "25000 25% (0m 37s) 1.2590 Aritza / Spanish ✓\n",
            "30000 30% (0m 45s) 2.6704 Inouye / French ✗ (Japanese)\n",
            "35000 35% (0m 53s) 0.0805 Arvanitoyannis / Greek ✓\n",
            "40000 40% (1m 0s) 1.3093 Demetrious / Greek ✓\n",
            "45000 45% (1m 8s) 1.5161 Senft / German ✓\n",
            "50000 50% (1m 16s) 0.0024 Baitchikov / Russian ✓\n",
            "55000 55% (1m 24s) 0.0858 Wojewodka / Polish ✓\n",
            "60000 60% (1m 31s) 2.0472 Dubicki / Italian ✗ (Polish)\n",
            "65000 65% (1m 39s) 0.8554 Ahn / Korean ✓\n",
            "70000 70% (1m 46s) 0.2407 Safar / Arabic ✓\n",
            "75000 75% (1m 54s) 3.3739 Toset / Dutch ✗ (Spanish)\n",
            "80000 80% (2m 1s) 3.8140 Gerges / German ✗ (Arabic)\n",
            "85000 85% (2m 9s) 0.2250 Ta / Vietnamese ✓\n",
            "90000 90% (2m 17s) 1.0029 Sakellariou / Greek ✓\n",
            "95000 95% (2m 24s) 0.1151 Toyama / Japanese ✓\n",
            "100000 100% (2m 32s) 2.7189 Holub / Arabic ✗ (Czech)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-ppl4s5QCkpR",
        "outputId": "c81d0338-fcf9-4d4e-ea61-8fe9189783be"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f36441c0908>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "KeRP_RnzCw5O",
        "outputId": "b3b6c77a-b030-4fb8-abd6-4d1c8eec241e"
      },
      "source": [
        "# Keep track of correct guesses in a confusion matrix\n",
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "# Just return an output given a line\n",
        "def evaluate(line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Go through a bunch of examples and record which are correctly guessed\n",
        "for i in range(n_confusion):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output = evaluate(line_tensor)\n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    category_i = all_categories.index(category)\n",
        "    confusion[category_i][guess_i] += 1\n",
        "\n",
        "# Normalize by dividing every row by its sum\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "# Set up plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set up axes\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Force label at every tick\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZr4BtknCyDv",
        "outputId": "4e719826-0722-487f-ae00-d740049498b1"
      },
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Dovesky\n",
            "(-0.91) Czech\n",
            "(-1.04) Russian\n",
            "(-2.01) English\n",
            "\n",
            "> Jackson\n",
            "(-0.08) Scottish\n",
            "(-2.84) English\n",
            "(-5.45) Russian\n",
            "\n",
            "> Satoshi\n",
            "(-1.04) Japanese\n",
            "(-1.41) Arabic\n",
            "(-1.69) Italian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj0RLLWUdmZc"
      },
      "source": [
        "- char 단위로 이름의 국적을 추적하는 것입니다.(목적)\n",
        "- 각 데이터 파일에는 각 언어별로 이름들이 포함되어 있으며 이를 `language: [names ...]}`의 형태로 만들어서 데이터를 준비합니다.(데이터 준비과정)\n",
        "- 신경망의 입력 값은 input과 초기 hidden을 합친 `combined`를 사용해서 두개의 선형 레이어를 통과합니다. 출력값은 `i2o`를 통과한 `output`과 `i2h`를 통과한 `hidden`을 합쳐서 생성됩니다.(네트워크 설명)\n",
        "- `categoryFromOutput`는 결과값에서 가장 큰 값의 인덱스를 사용해서 카테고리를 예측합니다. \n",
        "- 마지막 출력 값에 `nn.LogSoftmax`를 쓰기 때문에 오차 함수로는 `nn.NLLLoss()`를 사용합니다. \n",
        "- 훈련을 반복하면서 입력 값과 타겟 값을 텐서로 만들고, 0으로 초기화된 hidden state를 생성합니다. 이후 각 글자를 읽어오고 다음 글자를 위해서 hidden state은 유지합니다. 마지막 결과 값과 타겟 값을 비교한 후, 역전파를 진행합니다. (훈련 과정 설명)\n",
        "- 오차에 대한 그래프를 그리면 오차가 점차 감소하고 있는 것을 확인할 수 있습니다. 또한 confusion matrix를 그리면 각각의 카테고리에 대한 네트워크의 성능을 보여줍니다. 마지막으로 새로운 이름을 사용해서 예측한 결과를 볼 수 있습니다. (결과 해석)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkoebpwjDVQk"
      },
      "source": [
        "# 4번"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "1JBwoCTVC0UY",
        "outputId": "653fc8ca-0f97-4810-a5bd-3000ad44193c"
      },
      "source": [
        "!pip install torch<=1.2.0\n",
        "!pip install torchtext==0.04 \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: =1.2.0: No such file or directory\n",
            "Collecting torchtext==0.04\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.04) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.04) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.04) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.04) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.04) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.04) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.04) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.04) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.04) (1.24.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.04) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.04) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.04) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3ZOXqqyDYTD",
        "outputId": "bf78f229-d95c-44d7-bf71-4c7f8fd5f47b"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "NGRAMS = 2\n",
        "import os\n",
        "if not os.path.isdir('./.data'):\n",
        "\tos.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 43.9MB/s]\n",
            "120000lines [00:09, 13204.54lines/s]\n",
            "120000lines [00:19, 6130.23lines/s]\n",
            "7600lines [00:01, 6740.34lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdSw-1MUDeKm"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5e5rxGjEsG5"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGIbKffXEtrB"
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "    \n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBgCeHxLEu7Z"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj2yGcgMEyPE",
        "outputId": "f2855f00-ae31-46f3-9d69-e10c11e8b3e2"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "N_EPOCHS = 5\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 27 seconds\n",
            "\tLoss: 0.0263(train)\t|\tAcc: 84.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.0%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 27 seconds\n",
            "\tLoss: 0.0120(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 90.0%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 27 seconds\n",
            "\tLoss: 0.0070(train)\t|\tAcc: 96.3%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.3%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 27 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.1%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 27 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.1%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1gIumptEzz-",
        "outputId": "1c923672-e074-4f3a-b273-667217bffe0f"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0003(test)\t|\tAcc: 88.4%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8wmJBSHE2hv",
        "outputId": "609f788b-d0c3-4ef4-c92c-d96357238538"
      },
      "source": [
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rny6pmahnEpP"
      },
      "source": [
        "- 텍스트 분류 데이터 셋을 사용해서 텍스트의 내용을 카테고리 별로 분류하고자 합니다.(목적)\n",
        "- `ngrams`을 2로 설정해서 bi-grams문자열이 더해진 리스트를 생성합니다. (데이터 준비과정)\n",
        "- 모델은 `EmbeddingBag`레이어와 선형 레이어로 구성되어 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNmlIY9FFZHy"
      },
      "source": [
        "# 5번"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmynoXPE4Wn"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL3lRkBTGsWY",
        "outputId": "692747b3-0f96-453b-b4cf-b53bf7ccd697"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 951kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=4340166976090c2674cec05850ae5d3cc257a0f861c0515cc3315c6ebb2b90ba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sxu41m4q/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBR5BZRDFo_t",
        "outputId": "e9ac570b-af4c-4abc-b62a-08c7e2169769"
      },
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:04<00:00, 283kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 91.3kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 86.8kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CB24YvUHVff"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4Gya9WcFrAn"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrMjeauEHGwC",
        "outputId": "e6a8259b-6b05-4299-8e4e-0df0a4e5c9e9"
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # 디코더로의 첫 번째 입력은 <sos> 토큰입니다.\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ATTN_DIM = 64\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 1,856,685 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmK6rlV5HIki"
      },
      "source": [
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EKLW0rIVHZ0b",
        "outputId": "295a9e74-c882-4ddb-9693-09b2c6d586d9"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 7m 50s\n",
            "\tTrain Loss: 5.675 | Train PPL: 291.432\n",
            "\t Val. Loss: 5.244 |  Val. PPL: 189.365\n",
            "Epoch: 01 | Time: 7m 50s\n",
            "\tTrain Loss: 5.675 | Train PPL: 291.432\n",
            "\t Val. Loss: 5.244 |  Val. PPL: 189.365\n",
            "Epoch: 02 | Time: 7m 47s\n",
            "\tTrain Loss: 5.032 | Train PPL: 153.280\n",
            "\t Val. Loss: 5.133 |  Val. PPL: 169.518\n",
            "Epoch: 02 | Time: 7m 47s\n",
            "\tTrain Loss: 5.032 | Train PPL: 153.280\n",
            "\t Val. Loss: 5.133 |  Val. PPL: 169.518\n",
            "Epoch: 03 | Time: 7m 36s\n",
            "\tTrain Loss: 4.776 | Train PPL: 118.641\n",
            "\t Val. Loss: 4.982 |  Val. PPL: 145.788\n",
            "Epoch: 03 | Time: 7m 36s\n",
            "\tTrain Loss: 4.776 | Train PPL: 118.641\n",
            "\t Val. Loss: 4.982 |  Val. PPL: 145.788\n",
            "Epoch: 04 | Time: 7m 44s\n",
            "\tTrain Loss: 4.616 | Train PPL: 101.041\n",
            "\t Val. Loss: 5.113 |  Val. PPL: 166.162\n",
            "Epoch: 04 | Time: 7m 44s\n",
            "\tTrain Loss: 4.616 | Train PPL: 101.041\n",
            "\t Val. Loss: 5.113 |  Val. PPL: 166.162\n",
            "Epoch: 05 | Time: 7m 44s\n",
            "\tTrain Loss: 4.485 | Train PPL:  88.695\n",
            "\t Val. Loss: 5.070 |  Val. PPL: 159.114\n",
            "Epoch: 05 | Time: 7m 44s\n",
            "\tTrain Loss: 4.485 | Train PPL:  88.695\n",
            "\t Val. Loss: 5.070 |  Val. PPL: 159.114\n",
            "Epoch: 06 | Time: 7m 42s\n",
            "\tTrain Loss: 4.403 | Train PPL:  81.696\n",
            "\t Val. Loss: 4.956 |  Val. PPL: 142.074\n",
            "Epoch: 06 | Time: 7m 42s\n",
            "\tTrain Loss: 4.403 | Train PPL:  81.696\n",
            "\t Val. Loss: 4.956 |  Val. PPL: 142.074\n",
            "Epoch: 07 | Time: 7m 43s\n",
            "\tTrain Loss: 4.330 | Train PPL:  75.973\n",
            "\t Val. Loss: 4.856 |  Val. PPL: 128.533\n",
            "Epoch: 07 | Time: 7m 43s\n",
            "\tTrain Loss: 4.330 | Train PPL:  75.973\n",
            "\t Val. Loss: 4.856 |  Val. PPL: 128.533\n",
            "Epoch: 08 | Time: 7m 40s\n",
            "\tTrain Loss: 4.267 | Train PPL:  71.310\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.172\n",
            "Epoch: 08 | Time: 7m 40s\n",
            "\tTrain Loss: 4.267 | Train PPL:  71.310\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.172\n",
            "Epoch: 09 | Time: 7m 39s\n",
            "\tTrain Loss: 4.215 | Train PPL:  67.672\n",
            "\t Val. Loss: 4.868 |  Val. PPL: 130.019\n",
            "Epoch: 09 | Time: 7m 39s\n",
            "\tTrain Loss: 4.215 | Train PPL:  67.672\n",
            "\t Val. Loss: 4.868 |  Val. PPL: 130.019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1e5705e819b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1e5705e819b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n\u001b[0;32m--> 132\u001b[0;31m                                                           encoder_outputs)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_encoder_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36m_weighted_encoder_rep\u001b[0;34m(self, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                               encoder_outputs: Tensor) -> Tensor:\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mrepeated_decoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             encoder_outputs),\n\u001b[0;32m---> 72\u001b[0;31m             dim = 2)))\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1e5705e819b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1e5705e819b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n\u001b[0;32m--> 132\u001b[0;31m                                                           encoder_outputs)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_encoder_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36m_weighted_encoder_rep\u001b[0;34m(self, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                               encoder_outputs: Tensor) -> Tensor:\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1249038fd22a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mrepeated_decoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             encoder_outputs),\n\u001b[0;32m---> 72\u001b[0;31m             dim = 2)))\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnP0J4s1HbaY",
        "outputId": "f64ee3d8-30c0-4ddd-d2fc-785f59ce7b1a"
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 4.955 | Test PPL: 141.939 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4gYWrIKYidK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}