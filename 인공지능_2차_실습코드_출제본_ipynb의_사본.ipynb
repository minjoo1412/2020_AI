{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "인공지능 2차 실습코드 출제본.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjoo1412/2020_AI/blob/master/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_2%EC%B0%A8_%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C_%EC%B6%9C%EC%A0%9C%EB%B3%B8_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApMA_MsI4oHi",
        "outputId": "eef6e838-43eb-490b-f02a-0f7d2debea60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# Data loading\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "dataset = load_wine()\n",
        "data = dataset.data\n",
        "labels = dataset.target\n",
        "\n",
        "print(data)\n",
        "print(labels)\n",
        "print(data.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
            " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
            " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
            " ...\n",
            " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
            " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
            " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "(178, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi0SFzZs4zZP",
        "outputId": "6bd1cccc-4012-4d05-a66f-f3d8200ea48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Split data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3) \n",
        "print(len(x_train), len(x_test), type(x_train))\n",
        "\n",
        "print(x_test)\n",
        "print(y_test)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "124 54 <class 'numpy.ndarray'>\n",
            "[[1.410e+01 2.020e+00 2.400e+00 1.880e+01 1.030e+02 2.750e+00 2.920e+00\n",
            "  3.200e-01 2.380e+00 6.200e+00 1.070e+00 2.750e+00 1.060e+03]\n",
            " [1.184e+01 2.890e+00 2.230e+00 1.800e+01 1.120e+02 1.720e+00 1.320e+00\n",
            "  4.300e-01 9.500e-01 2.650e+00 9.600e-01 2.520e+00 5.000e+02]\n",
            " [1.233e+01 1.100e+00 2.280e+00 1.600e+01 1.010e+02 2.050e+00 1.090e+00\n",
            "  6.300e-01 4.100e-01 3.270e+00 1.250e+00 1.670e+00 6.800e+02]\n",
            " [1.207e+01 2.160e+00 2.170e+00 2.100e+01 8.500e+01 2.600e+00 2.650e+00\n",
            "  3.700e-01 1.350e+00 2.760e+00 8.600e-01 3.280e+00 3.780e+02]\n",
            " [1.369e+01 3.260e+00 2.540e+00 2.000e+01 1.070e+02 1.830e+00 5.600e-01\n",
            "  5.000e-01 8.000e-01 5.880e+00 9.600e-01 1.820e+00 6.800e+02]\n",
            " [1.217e+01 1.450e+00 2.530e+00 1.900e+01 1.040e+02 1.890e+00 1.750e+00\n",
            "  4.500e-01 1.030e+00 2.950e+00 1.450e+00 2.230e+00 3.550e+02]\n",
            " [1.375e+01 1.730e+00 2.410e+00 1.600e+01 8.900e+01 2.600e+00 2.760e+00\n",
            "  2.900e-01 1.810e+00 5.600e+00 1.150e+00 2.900e+00 1.320e+03]\n",
            " [1.305e+01 2.050e+00 3.220e+00 2.500e+01 1.240e+02 2.630e+00 2.680e+00\n",
            "  4.700e-01 1.920e+00 3.580e+00 1.130e+00 3.200e+00 8.300e+02]\n",
            " [1.382e+01 1.750e+00 2.420e+00 1.400e+01 1.110e+02 3.880e+00 3.740e+00\n",
            "  3.200e-01 1.870e+00 7.050e+00 1.010e+00 3.260e+00 1.190e+03]\n",
            " [1.336e+01 2.560e+00 2.350e+00 2.000e+01 8.900e+01 1.400e+00 5.000e-01\n",
            "  3.700e-01 6.400e-01 5.600e+00 7.000e-01 2.470e+00 7.800e+02]\n",
            " [1.285e+01 3.270e+00 2.580e+00 2.200e+01 1.060e+02 1.650e+00 6.000e-01\n",
            "  6.000e-01 9.600e-01 5.580e+00 8.700e-01 2.110e+00 5.700e+02]\n",
            " [1.317e+01 5.190e+00 2.320e+00 2.200e+01 9.300e+01 1.740e+00 6.300e-01\n",
            "  6.100e-01 1.550e+00 7.900e+00 6.000e-01 1.480e+00 7.250e+02]\n",
            " [1.386e+01 1.510e+00 2.670e+00 2.500e+01 8.600e+01 2.950e+00 2.860e+00\n",
            "  2.100e-01 1.870e+00 3.380e+00 1.360e+00 3.160e+00 4.100e+02]\n",
            " [1.281e+01 2.310e+00 2.400e+00 2.400e+01 9.800e+01 1.150e+00 1.090e+00\n",
            "  2.700e-01 8.300e-01 5.700e+00 6.600e-01 1.360e+00 5.600e+02]\n",
            " [1.434e+01 1.680e+00 2.700e+00 2.500e+01 9.800e+01 2.800e+00 1.310e+00\n",
            "  5.300e-01 2.700e+00 1.300e+01 5.700e-01 1.960e+00 6.600e+02]\n",
            " [1.352e+01 3.170e+00 2.720e+00 2.350e+01 9.700e+01 1.550e+00 5.200e-01\n",
            "  5.000e-01 5.500e-01 4.350e+00 8.900e-01 2.060e+00 5.200e+02]\n",
            " [1.156e+01 2.050e+00 3.230e+00 2.850e+01 1.190e+02 3.180e+00 5.080e+00\n",
            "  4.700e-01 1.870e+00 6.000e+00 9.300e-01 3.690e+00 4.650e+02]\n",
            " [1.251e+01 1.240e+00 2.250e+00 1.750e+01 8.500e+01 2.000e+00 5.800e-01\n",
            "  6.000e-01 1.250e+00 5.450e+00 7.500e-01 1.510e+00 6.500e+02]\n",
            " [1.328e+01 1.640e+00 2.840e+00 1.550e+01 1.100e+02 2.600e+00 2.680e+00\n",
            "  3.400e-01 1.360e+00 4.600e+00 1.090e+00 2.780e+00 8.800e+02]\n",
            " [1.260e+01 2.460e+00 2.200e+00 1.850e+01 9.400e+01 1.620e+00 6.600e-01\n",
            "  6.300e-01 9.400e-01 7.100e+00 7.300e-01 1.580e+00 6.950e+02]\n",
            " [1.285e+01 1.600e+00 2.520e+00 1.780e+01 9.500e+01 2.480e+00 2.370e+00\n",
            "  2.600e-01 1.460e+00 3.930e+00 1.090e+00 3.630e+00 1.015e+03]\n",
            " [1.225e+01 1.730e+00 2.120e+00 1.900e+01 8.000e+01 1.650e+00 2.030e+00\n",
            "  3.700e-01 1.630e+00 3.400e+00 1.000e+00 3.170e+00 5.100e+02]\n",
            " [1.305e+01 5.800e+00 2.130e+00 2.150e+01 8.600e+01 2.620e+00 2.650e+00\n",
            "  3.000e-01 2.010e+00 2.600e+00 7.300e-01 3.100e+00 3.800e+02]\n",
            " [1.184e+01 8.900e-01 2.580e+00 1.800e+01 9.400e+01 2.200e+00 2.210e+00\n",
            "  2.200e-01 2.350e+00 3.050e+00 7.900e-01 3.080e+00 5.200e+02]\n",
            " [1.229e+01 3.170e+00 2.210e+00 1.800e+01 8.800e+01 2.850e+00 2.990e+00\n",
            "  4.500e-01 2.810e+00 2.300e+00 1.420e+00 2.830e+00 4.060e+02]\n",
            " [1.216e+01 1.610e+00 2.310e+00 2.280e+01 9.000e+01 1.780e+00 1.690e+00\n",
            "  4.300e-01 1.560e+00 2.450e+00 1.330e+00 2.260e+00 4.950e+02]\n",
            " [1.272e+01 1.750e+00 2.280e+00 2.250e+01 8.400e+01 1.380e+00 1.760e+00\n",
            "  4.800e-01 1.630e+00 3.300e+00 8.800e-01 2.420e+00 4.880e+02]\n",
            " [1.103e+01 1.510e+00 2.200e+00 2.150e+01 8.500e+01 2.460e+00 2.170e+00\n",
            "  5.200e-01 2.010e+00 1.900e+00 1.710e+00 2.870e+00 4.070e+02]\n",
            " [1.350e+01 1.810e+00 2.610e+00 2.000e+01 9.600e+01 2.530e+00 2.610e+00\n",
            "  2.800e-01 1.660e+00 3.520e+00 1.120e+00 3.820e+00 8.450e+02]\n",
            " [1.208e+01 2.080e+00 1.700e+00 1.750e+01 9.700e+01 2.230e+00 2.170e+00\n",
            "  2.600e-01 1.400e+00 3.300e+00 1.270e+00 2.960e+00 7.100e+02]\n",
            " [1.383e+01 1.570e+00 2.620e+00 2.000e+01 1.150e+02 2.950e+00 3.400e+00\n",
            "  4.000e-01 1.720e+00 6.600e+00 1.130e+00 2.570e+00 1.130e+03]\n",
            " [1.237e+01 1.170e+00 1.920e+00 1.960e+01 7.800e+01 2.110e+00 2.000e+00\n",
            "  2.700e-01 1.040e+00 4.680e+00 1.120e+00 3.480e+00 5.100e+02]\n",
            " [1.200e+01 3.430e+00 2.000e+00 1.900e+01 8.700e+01 2.000e+00 1.640e+00\n",
            "  3.700e-01 1.870e+00 1.280e+00 9.300e-01 3.050e+00 5.640e+02]\n",
            " [1.390e+01 1.680e+00 2.120e+00 1.600e+01 1.010e+02 3.100e+00 3.390e+00\n",
            "  2.100e-01 2.140e+00 6.100e+00 9.100e-01 3.330e+00 9.850e+02]\n",
            " [1.339e+01 1.770e+00 2.620e+00 1.610e+01 9.300e+01 2.850e+00 2.940e+00\n",
            "  3.400e-01 1.450e+00 4.800e+00 9.200e-01 3.220e+00 1.195e+03]\n",
            " [1.410e+01 2.160e+00 2.300e+00 1.800e+01 1.050e+02 2.950e+00 3.320e+00\n",
            "  2.200e-01 2.380e+00 5.750e+00 1.250e+00 3.170e+00 1.510e+03]\n",
            " [1.340e+01 4.600e+00 2.860e+00 2.500e+01 1.120e+02 1.980e+00 9.600e-01\n",
            "  2.700e-01 1.110e+00 8.500e+00 6.700e-01 1.920e+00 6.300e+02]\n",
            " [1.264e+01 1.360e+00 2.020e+00 1.680e+01 1.000e+02 2.020e+00 1.410e+00\n",
            "  5.300e-01 6.200e-01 5.750e+00 9.800e-01 1.590e+00 4.500e+02]\n",
            " [1.237e+01 1.630e+00 2.300e+00 2.450e+01 8.800e+01 2.220e+00 2.450e+00\n",
            "  4.000e-01 1.900e+00 2.120e+00 8.900e-01 2.780e+00 3.420e+02]\n",
            " [1.222e+01 1.290e+00 1.940e+00 1.900e+01 9.200e+01 2.360e+00 2.040e+00\n",
            "  3.900e-01 2.080e+00 2.700e+00 8.600e-01 3.020e+00 3.120e+02]\n",
            " [1.245e+01 3.030e+00 2.640e+00 2.700e+01 9.700e+01 1.900e+00 5.800e-01\n",
            "  6.300e-01 1.140e+00 7.500e+00 6.700e-01 1.730e+00 8.800e+02]\n",
            " [1.176e+01 2.680e+00 2.920e+00 2.000e+01 1.030e+02 1.750e+00 2.030e+00\n",
            "  6.000e-01 1.050e+00 3.800e+00 1.230e+00 2.500e+00 6.070e+02]\n",
            " [1.419e+01 1.590e+00 2.480e+00 1.650e+01 1.080e+02 3.300e+00 3.930e+00\n",
            "  3.200e-01 1.860e+00 8.700e+00 1.230e+00 2.820e+00 1.680e+03]\n",
            " [1.208e+01 1.130e+00 2.510e+00 2.400e+01 7.800e+01 2.000e+00 1.580e+00\n",
            "  4.000e-01 1.400e+00 2.200e+00 1.310e+00 2.720e+00 6.300e+02]\n",
            " [1.387e+01 1.900e+00 2.800e+00 1.940e+01 1.070e+02 2.950e+00 2.970e+00\n",
            "  3.700e-01 1.760e+00 4.500e+00 1.250e+00 3.400e+00 9.150e+02]\n",
            " [1.323e+01 3.300e+00 2.280e+00 1.850e+01 9.800e+01 1.800e+00 8.300e-01\n",
            "  6.100e-01 1.870e+00 1.052e+01 5.600e-01 1.510e+00 6.750e+02]\n",
            " [1.383e+01 1.650e+00 2.600e+00 1.720e+01 9.400e+01 2.450e+00 2.990e+00\n",
            "  2.200e-01 2.290e+00 5.600e+00 1.240e+00 3.370e+00 1.265e+03]\n",
            " [1.388e+01 5.040e+00 2.230e+00 2.000e+01 8.000e+01 9.800e-01 3.400e-01\n",
            "  4.000e-01 6.800e-01 4.900e+00 5.800e-01 1.330e+00 4.150e+02]\n",
            " [1.164e+01 2.060e+00 2.460e+00 2.160e+01 8.400e+01 1.950e+00 1.690e+00\n",
            "  4.800e-01 1.350e+00 2.800e+00 1.000e+00 2.750e+00 6.800e+02]\n",
            " [1.145e+01 2.400e+00 2.420e+00 2.000e+01 9.600e+01 2.900e+00 2.790e+00\n",
            "  3.200e-01 1.830e+00 3.250e+00 8.000e-01 3.390e+00 6.250e+02]\n",
            " [1.258e+01 1.290e+00 2.100e+00 2.000e+01 1.030e+02 1.480e+00 5.800e-01\n",
            "  5.300e-01 1.400e+00 7.600e+00 5.800e-01 1.550e+00 6.400e+02]\n",
            " [1.438e+01 3.590e+00 2.280e+00 1.600e+01 1.020e+02 3.250e+00 3.170e+00\n",
            "  2.700e-01 2.190e+00 4.900e+00 1.040e+00 3.440e+00 1.065e+03]\n",
            " [1.330e+01 1.720e+00 2.140e+00 1.700e+01 9.400e+01 2.400e+00 2.190e+00\n",
            "  2.700e-01 1.350e+00 3.950e+00 1.020e+00 2.770e+00 1.285e+03]\n",
            " [1.358e+01 2.580e+00 2.690e+00 2.450e+01 1.050e+02 1.550e+00 8.400e-01\n",
            "  3.900e-01 1.540e+00 8.660e+00 7.400e-01 1.800e+00 7.500e+02]]\n",
            "[0 1 1 1 2 1 0 0 0 2 2 2 1 2 2 2 1 2 0 2 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 2\n",
            " 1 1 1 2 1 0 1 0 2 0 2 1 1 2 0 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkfYVX555tVh",
        "outputId": "e1c726da-e6cc-4f9f-9297-5a70090de6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Convert to tensor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "print(type(x_train))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JILCuCqf6USv"
      },
      "source": [
        "# Generating dataset\n",
        "\n",
        "train_set = TensorDataset(x_train, y_train)\n",
        "test_set = TensorDataset(x_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "문제 1 : train_loader 생성 ( 5점 )\n",
        "\n",
        "조건 1 : train_set 변수 활용\n",
        "조건 2 : 배치 크기는 16\n",
        "조건 3 : shuffle 사용\n",
        "\"\"\"\n",
        "train_loader = DataLoader(dataset=test_set, batch_size=16, shuffle=True, num_workers=2)\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R13TlkArFqzi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umKKUzh16esj",
        "outputId": "afbe4870-3cf3-4d41-e2f6-60429e3d5c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Construct model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    \n",
        "    self.fc1 = nn.Linear(13, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 16)\n",
        "    self.fc6 = nn.Linear(16, 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.relu(self.fc5(x))\n",
        "    x = F.relu(self.fc6(x))\n",
        "    x = self.softmax(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "model = Model()\n",
        "print(model)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (fc1): Linear(in_features=13, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc5): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (fc6): Linear(in_features=16, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (softmax): Softmax(dim=0)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO2QwNMl6gVt"
      },
      "source": [
        "# Configure optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "\"\"\"\n",
        "문제 4 : Optimizer 생성 (5점)\n",
        "\n",
        "조건 1 : SGD 생성\n",
        "조건 2 : 학습률 0.001\n",
        "조건 3 : Momentum 0.9\n",
        "\"\"\"\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)### 코드 작성 위치\n",
        "\n",
        "\"\"\"\n",
        "문제 5 : 손실 함수 생성 (5점 )\n",
        "\n",
        "조건 1 : Cross Entropy Loss 생성\n",
        "\"\"\"\n",
        "criterion = nn.CrossEntropyLoss()### 코드 작성 위치"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMF-ZKi6DFaH",
        "outputId": "6bc633e7-8c16-4bd5-9acd-56ab0320ae6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# Training\n",
        "\n",
        "epochs = 200\n",
        "losses = list()\n",
        "accuracies = list()\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0  \n",
        "  epoch_accuracy = 0\n",
        "\n",
        "\n",
        "  for x, y in train_loader:\n",
        "    output = model(x_train)\n",
        "\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    accuracy = (predicted == y).sum().item()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_accuracy += accuracy\n",
        "  \n",
        "\n",
        "  epoch_loss /= len(train_loader)\n",
        "  epoch_accuracy /= len(x_train)\n",
        "  print(\"epoch :{}, \\tloss :{}, \\taccuracy :{}\".format(str(epoch+1).zfill(3),round(epoch_loss,4), round(epoch_accuracy,4)))\n",
        "  \n",
        "  losses.append(epoch_loss)\n",
        "  accuracies.append(epoch_accuracy)\n",
        "\n",
        "  "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-99ef4b03cfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (124) must match the size of tensor b (16) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJiOlSqxGQnl",
        "outputId": "03d23de9-ebd3-4097-feb7-98ca37cb1a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "# Plot result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"$loss$\",fontsize = 18)\n",
        "plt.plot(losses)\n",
        "plt.grid()\n",
        "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"$accuracy$\", fontsize = 18)\n",
        "plt.plot(accuracies)\n",
        "plt.grid()\n",
        "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFstYt_tmwlU",
        "outputId": "540b9d2a-abfa-46de-e3b4-93a659a75a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# Test\n",
        "\n",
        "output = model(x_test)\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "accuracy = round((predicted == y_test).sum().item() / len(y_test),4)\n",
        "\n",
        "\n",
        "print(\"test_set accuracy :\", round(accuracy,4))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-78e63d6aa4e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-3adb3d9db9d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [54 x 13], m2: [208 x 256] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    }
  ]
}